"""
Stage 3: Assign Page Numbers to Sections.

Purpose:
Processes section JSON files generated by Stage 2. For each section, it locates
the corresponding original chapter markdown file (from the initial conversion,
e.g., `mdsplitkit` output). It then reads this original markdown file to extract
the `<!-- PageNumber="X" -->` tags and builds a mapping of character positions
to page numbers. Using this mapping and the section's start/end character
positions (relative to the original chapter content), it determines the start
and end page numbers for the section. Sections where no page tags fall within
their character range inherit the end page of the preceding section. The updated
section data, now including `section_page_start` and `section_page_end`, is
saved back to a new JSON file in the output directory.

Input: Section JSON files from `INPUT_DIR` (Stage 2 output).
       Original chapter markdown files in `ORIGINAL_MD_DIR`.
Output: Updated section JSON files (with page numbers) in `OUTPUT_DIR`.
"""

import os
import json
import traceback  # Retained for detailed error logging
import re
import bisect  # Used for efficient searching in page mapping
from collections import defaultdict  # Used for grouping sections by chapter

try:
    import natsort
except ImportError:
    natsort = None  # Optional dependency for natural sorting.

try:
    from tqdm import tqdm
except ImportError:
    tqdm = None  # Optional dependency for progress bar.


# --- Configuration & Constants ---
INPUT_DIR = "2B_merged_sections"  # Directory containing Stage 2 section JSON files.
OUTPUT_DIR = "2C_paged_sections"  # Directory to save updated section JSON files with page numbers.
ORIGINAL_MD_DIR = (
    "1C_mdsplitkit_output"  # Directory containing the original source markdown files.
)

# Regex to find page number tags like <!-- PageNumber="123" -->.
PAGE_NUMBER_TAG_PATTERN = re.compile(r'<!--\s*PageNumber="(\d+)"\s*-->')


# --- Utility Functions (Mostly Inlined/Duplicated - Consider Refactoring) ---


def extract_page_mapping(content: str) -> list[tuple[int, int]]:
    """
    Extracts a mapping of character positions to page numbers from tags.
    (Identical logic to the function in 0_inspect_pdf_pages.py and others)
    """
    # --- This function's implementation is identical to the one in ---
    # --- previous stages. Consider refactoring into a shared utils module. ---
    mapping = []
    raw_matches = []  # Store raw matches first

    # 1. Find all tags and their positions/page numbers
    for match in PAGE_NUMBER_TAG_PATTERN.finditer(content):
        pos = match.start()
        page_num = int(match.group(1))
        raw_matches.append((pos, page_num))

    if not raw_matches:
        return []  # No tags found

    # 2. Create initial mapping, resolving duplicate positions by keeping higher page number
    if raw_matches:
        # Sort primarily by position, secondarily by page number (desc) to prioritize higher pages at same pos
        raw_matches.sort(key=lambda x: (x[0], -x[1]))

        mapping.append(raw_matches[0])  # Add the first match
        for i in range(1, len(raw_matches)):
            # Add if the position is different from the last added position
            if raw_matches[i][0] > mapping[-1][0]:
                mapping.append(raw_matches[i])
            # If position is the same, the sort already ensured the one with the highest
            # page number came first, so we skip subsequent matches at the same position.

    # 3. Ensure the mapping covers the end of the document
    if mapping:
        last_entry_pos, last_entry_page = mapping[-1]
        if last_entry_pos < len(content):
            if not mapping or mapping[-1][0] < len(content):
                mapping.append((len(content), last_entry_page))
            elif mapping[-1][0] == len(content):
                mapping[-1] = (len(content), max(last_entry_page, mapping[-1][1]))

    return mapping


def get_page_range(
    start_pos: int, end_pos: int, page_mapping: list[tuple[int, int]]
) -> tuple[int | None, int | None]:
    """
    Determines the start and end page numbers for a text section based on its
    character positions within the original content and a pre-computed page mapping.

    Uses binary search (`bisect`) on the `page_mapping` (list of (position, page_num)
    tuples sorted by position) to find the relevant page numbers.

    Crucially, it checks if *any* `PageNumber` tag actually falls within the
    section's character range (`start_pos` <= tag_pos < `end_pos`). If no tag
    is found within this specific range, it returns (None, None), indicating
    that the page numbers should be inferred from the previous section. Otherwise,
    it returns the calculated start and end pages based on the tags surrounding
    or within the section.

    Args:
        start_pos: The starting character position of the section in the original content.
        end_pos: The ending character position of the section (exclusive).
        page_mapping: The sorted list of (position, page_number) tuples extracted
                      from the original chapter markdown.

    Returns:
        A tuple (page_start, page_end) if a page tag exists within the section's
        range, otherwise (None, None).
    """
    if not page_mapping:  # No tags in the entire chapter
        return None, None

    # --- Determine potential start and end pages based on positions ---

    # Find the index of the page mapping entry *just before or at* the start_pos
    # bisect_right finds insertion point; subtract 1 to get the index of the element <= start_pos
    start_idx = bisect.bisect_right(page_mapping, (start_pos,)) - 1
    # If start_pos is before the first tag, default to page 1, otherwise use the found tag's page.
    # If start_idx is -1, it means start_pos is before the first tag's position.
    # We might default to page 1 or the first tag's page depending on interpretation.
    # Let's use the page number of the tag immediately preceding or at start_pos.
    # If start_idx is -1, it implies the section starts before any tag, assume page 1 or first tag's page.
    if start_idx < 0:
        # If the first tag isn't at pos 0, content before it might be page 1.
        # However, using the first tag's page number is safer if page numbering doesn't start at 1.
        potential_page_start = page_mapping[0][
            1
        ]  # Use the first page number found in the doc
    else:
        potential_page_start = page_mapping[start_idx][1]

    # Find the index for the end_pos. Use end_pos - 1 because the range is exclusive [start, end).
    # We want the page number active at the *last character* of the section.
    effective_end_pos = max(0, end_pos - 1)  # Ensure not negative
    end_idx = bisect.bisect_right(page_mapping, (effective_end_pos,)) - 1
    # If end_pos is before the first tag, use the same logic as start page.
    if end_idx < 0:
        potential_page_end = page_mapping[0][1]
    else:
        potential_page_end = page_mapping[end_idx][1]

    # Ensure page end is not logically before page start
    potential_page_end = max(potential_page_start, potential_page_end)

    # --- Check if any tag falls within the section's actual range ---
    tag_found_within_range = False
    for tag_pos, _ in page_mapping:
        # Check if the tag's position is within the section's bounds [start_pos, end_pos)
        if start_pos <= tag_pos < end_pos:
            tag_found_within_range = True
            break

    # If no tag was found strictly within the section's character range,
    # return (None, None) to signal that the page should be inferred.
    if not tag_found_within_range:
        return None, None
    else:
        # If a tag *was* found within the range, return the calculated pages.
        return potential_page_start, potential_page_end


def create_directory(directory: str):
    """Creates the specified directory if it does not already exist."""
    os.makedirs(directory, exist_ok=True)


# --- Core Logic ---


def find_original_md(source_md_filename: str, original_md_dir: str) -> str | None:
    """
    Locates the original source markdown file within the specified directory,
    searching subdirectories if necessary.

    Args:
        source_md_filename: The base name of the markdown file to find.
        original_md_dir: The root directory to search within.

    Returns:
        The full path to the found file, or None if not found.
    """
    # Check the top-level directory first
    direct_path = os.path.join(original_md_dir, source_md_filename)
    if os.path.exists(direct_path):
        return direct_path

    # If not found directly, search recursively
    print(
        f"  INFO: '{source_md_filename}' not in top level of '{original_md_dir}'. Searching subdirs..."
    )
    try:
        for root, _, files in os.walk(original_md_dir):
            if source_md_filename in files:
                found_path = os.path.join(root, source_md_filename)
                print(f"  Found original MD file at: {found_path}")
                return found_path
    except Exception as e:
        print(
            f"  ERROR during subdirectory search for '{source_md_filename}' in '{original_md_dir}': {e}"
        )
        return None

    # If not found after searching
    print(
        f"  ERROR: Could not locate '{source_md_filename}' within '{original_md_dir}' or its subdirectories."
    )
    return None


def group_sections_by_chapter(input_dir: str) -> dict[str, list[dict]] | None:
    """
    Scans the input directory for section JSON files and groups them by their
    'source_markdown_file' property.

    Args:
        input_dir: The directory containing section JSON files (Stage 2 output).

    Returns:
        A dictionary where keys are source markdown filenames and values are lists
        of dictionaries, each containing the 'filename' and loaded 'data' of a
        section JSON belonging to that source file. Returns None on critical error
        (e.g., input directory not found). Returns empty dict if directory exists
        but contains no valid JSONs.
    """
    sections_by_chapter = defaultdict(list)
    all_json_filenames = []
    try:
        # List all files in the input directory
        all_files = os.listdir(input_dir)
        # Filter for JSON files
        all_json_filenames = [f for f in all_files if f.endswith(".json")]
    except FileNotFoundError:
        print(f"ERROR: Input directory not found: {input_dir}")
        return None  # Critical error
    except OSError as e:
        print(f"ERROR: Could not list files in input directory '{input_dir}': {e}")
        return None  # Critical error

    if not all_json_filenames:
        print(f"No section JSON files found in {input_dir}.")
        return {}  # Not an error, but nothing to process

    # Sort filenames (natural sort preferred)
    if natsort:
        all_json_filenames = natsort.natsorted(all_json_filenames)
        print(
            f"Found and naturally sorted {len(all_json_filenames)} section JSON files."
        )
    else:
        all_json_filenames.sort()
        print(f"Found {len(all_json_filenames)} section JSON files (standard sort).")
        if natsort is None:
            print(
                "INFO: Install 'natsort' for potentially better file ordering (pip install natsort)."
            )

    # Load JSON data and group by source markdown file
    print("Grouping sections by source chapter...")
    files_missing_source = 0
    files_read_error = 0
    for filename in all_json_filenames:
        filepath = os.path.join(input_dir, filename)
        try:
            with open(filepath, "r", encoding="utf-8") as f:
                section_data = json.load(f)

            source_md = section_data.get("source_markdown_file")
            if source_md:
                # Append dict containing filename and loaded data
                sections_by_chapter[source_md].append(
                    {"filename": filename, "data": section_data}
                )
            else:
                print(
                    f"  WARN: Missing 'source_markdown_file' key in {filename}. Skipping."
                )
                files_missing_source += 1
        except (json.JSONDecodeError, OSError, Exception) as e:
            print(f"  ERROR reading or parsing {filename}: {e}. Skipping.")
            files_read_error += 1

    if files_missing_source > 0:
        print(
            f"Warning: {files_missing_source} files were skipped due to missing 'source_markdown_file'."
        )
    if files_read_error > 0:
        print(f"Warning: {files_read_error} files failed to load or parse.")

    print(f"Grouped sections into {len(sections_by_chapter)} source chapter(s).")
    return sections_by_chapter


def process_chapter_group(
    source_md_filename: str, sections: list[dict], original_md_dir: str, output_dir: str
) -> tuple[int, int]:
    """
    Processes all sections associated with a single source markdown file.

    Loads the original markdown, extracts page mapping, calculates page ranges
    for each section, and saves the updated section data.

    Args:
        source_md_filename: The filename of the original source markdown chapter.
        sections: A list of dictionaries, each containing 'filename' and 'data'
                  for a section belonging to this chapter.
        original_md_dir: The directory containing the original markdown files.
        output_dir: The directory to save the updated section JSON files.

    Returns:
        A tuple: (number_of_sections_successfully_processed, number_of_sections_failed).
    """
    print(
        f"\nProcessing chapter group: {source_md_filename} ({len(sections)} sections)"
    )
    processed_count = 0
    failed_count = 0

    # 1. Find and load the original markdown content for this chapter
    original_md_path = find_original_md(source_md_filename, original_md_dir)
    if not original_md_path:
        print(
            f"  ERROR: Original MD file not found. Skipping all sections for this chapter."
        )
        return 0, len(sections)  # 0 processed, all failed

    try:
        with open(original_md_path, "r", encoding="utf-8") as f:
            raw_chapter_content = f.read()
    except Exception as e:
        print(
            f"  ERROR: Failed to read original MD file '{original_md_path}': {e}. Skipping chapter."
        )
        return 0, len(sections)

    # 2. Extract page mapping from the original content
    page_mapping = extract_page_mapping(raw_chapter_content)
    if not page_mapping:
        print(
            f"  WARN: No page number tags found in original MD file '{original_md_path}'. Page numbers may be inaccurate."
        )
    else:
        print(
            f"  Extracted page mapping from '{original_md_path}' ({len(page_mapping)} tags)."
        )

    # 3. Sort sections by their start position to process them in order
    try:
        sections.sort(
            key=lambda s: s["data"].get("start_pos", float("inf"))
        )  # Sort, put sections without start_pos last
    except Exception as e:
        print(
            f"  WARN: Could not sort sections for {source_md_filename} due to error: {e}. Processing in potentially incorrect order."
        )

    # 4. Process each section in the group
    # Initialize with a sensible default (e.g., 1) or potentially the first page found in mapping
    previous_section_end_page = page_mapping[0][1] if page_mapping else 1

    for section_info in sections:
        section_data = section_info["data"]
        section_filename = section_info[
            "filename"
        ]  # Original JSON filename from Stage 2
        output_filepath = os.path.join(
            output_dir, section_filename
        )  # Save with the same name

        start_pos = section_data.get("start_pos")
        end_pos = section_data.get("end_pos")

        # Check for necessary position data
        if start_pos is None or end_pos is None:
            print(
                f"  ERROR: Section {section_filename} is missing 'start_pos' or 'end_pos'. Cannot determine page range. Skipping."
            )
            failed_count += 1
            continue

        try:
            # Get page range based on tags within the section's span
            page_range_result = get_page_range(start_pos, end_pos, page_mapping)

            if page_range_result == (None, None):
                # No tags within range - infer based on previous section's end page
                section_page_start = previous_section_end_page
                section_page_end = previous_section_end_page
                print(
                    f"  {section_filename}: No tags in range [{start_pos}-{end_pos}). Inferred page(s): {section_page_start}"
                )
            else:
                # Tags found - use the calculated range
                section_page_start, section_page_end = page_range_result
                print(
                    f"  {section_filename}: Tags found in range. Calculated page range: {section_page_start}-{section_page_end}"
                )

            # Update section data with the determined page numbers
            section_data["section_page_start"] = section_page_start
            section_data["section_page_end"] = section_page_end

            # Update the end page for the *next* section's potential fallback
            previous_section_end_page = section_page_end

            # Save the updated section data back to its JSON file
            with open(output_filepath, "w", encoding="utf-8") as f:
                json.dump(section_data, f, indent=2, ensure_ascii=False)
            processed_count += 1

        except Exception as e:
            print(f"\nERROR processing section {section_filename} in Stage 3: {e}")
            print(traceback.format_exc())
            failed_count += 1

    print(
        f"  Finished chapter group {source_md_filename}: Processed={processed_count}, Failed={failed_count}"
    )
    return processed_count, failed_count


def main():
    """
    Main execution function for Stage 3.

    Groups section JSON files by their source chapter, then processes each group
    to determine and add page number information based on the original markdown files.
    Saves the updated section JSON files to the output directory.
    """
    print("-" * 50)
    print("Running Stage 3: Assign Page Numbers to Sections")
    print(f"Input section JSON directory: {INPUT_DIR}")
    print(f"Original MD directory       : {ORIGINAL_MD_DIR}")
    print(f"Output paged JSON directory : {OUTPUT_DIR}")
    print("-" * 50)

    # Ensure output directory exists
    create_directory(OUTPUT_DIR)

    # 1. Group section JSON files by their source markdown chapter
    sections_by_chapter = group_sections_by_chapter(INPUT_DIR)

    # Exit if grouping failed critically or found no sections
    if sections_by_chapter is None:
        print("Exiting due to error during section grouping.")
        return
    if not sections_by_chapter:
        # Message already printed by group_sections_by_chapter
        return

    # 2. Process each chapter group sequentially
    total_processed_sections = 0
    total_failed_sections = 0

    # Setup progress bar if tqdm is available
    chapter_iterator = sections_by_chapter.items()
    if tqdm:
        chapter_iterator = tqdm(
            chapter_iterator,
            desc="Stage 3 Processing Chapters",
            unit="chapter",
            ncols=100,
        )

    # Iterate through chapters (groups of sections)
    for source_md_filename, sections_in_group in chapter_iterator:
        processed_in_group, failed_in_group = process_chapter_group(
            source_md_filename, sections_in_group, ORIGINAL_MD_DIR, OUTPUT_DIR
        )
        total_processed_sections += processed_in_group
        total_failed_sections += failed_in_group

    # 3. Print final summary
    print("-" * 50)
    print("Stage 3 Summary:")
    print(f"Total sections processed successfully: {total_processed_sections}")
    print(f"Total sections failed processing   : {total_failed_sections}")
    print(f"Output paged section JSON files are in: {OUTPUT_DIR}")
    print("-" * 50)


if __name__ == "__main__":
    main()
