# -*- coding: utf-8 -*-
"""
Stage 2: Section Identification & Enrichment

Purpose:
Processes chapter data generated by Stage 1. It identifies logical sections
within each chapter based on Markdown headings (H1-H6). For each section, it
calculates token count, determines start/end page numbers, and uses an LLM
to generate a concise summary, tags, applicable standard(s), standard codes,
an importance score, and references.

Input: JSON file from Stage 1 (e.g., 'pipeline_output/stage1/stage1_chapter_data.json').
Output: A JSON file in OUTPUT_DIR containing a list of section dictionaries.
        (e.g., 'pipeline_output/stage2/stage2_section_data.json')
"""

import os
import json
import traceback
import re
import time
import logging
import requests
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional, Union
from collections import defaultdict

# --- Dependencies Check ---
try:
    import tiktoken
except ImportError:
    tiktoken = None
    print("WARNING: tiktoken not installed. Token counts will be estimated (chars/4). `pip install tiktoken`")

try:
    import natsort
except ImportError:
    natsort = None
    print("INFO: natsort not installed. Chapters/Sections might not sort naturally. `pip install natsort`")

try:
    from openai import OpenAI, APIError
except ImportError:
    OpenAI = None
    APIError = None
    print("ERROR: openai library not installed. GPT features unavailable. `pip install openai`")

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x # Make tqdm optional
    print("INFO: tqdm not installed. Progress bars disabled. `pip install tqdm`")

# ==============================================================================
# Configuration
# ==============================================================================

# --- Directory Paths ---
# TODO: Adjust these paths as needed
STAGE1_OUTPUT_DIR = "pipeline_output/stage1"
STAGE1_FILENAME = "stage1_chapter_data.json"
OUTPUT_DIR = "pipeline_output/stage2"
OUTPUT_FILENAME = "stage2_section_data.json"
LOG_DIR = "pipeline_output/logs"

# --- API Configuration ---
# TODO: Load securely or replace placeholders (Ensure consistency with Stage 1)
BASE_URL = os.environ.get("OPENAI_API_BASE", "https://api.example.com/v1")
MODEL_NAME_CHAT = os.environ.get("OPENAI_MODEL_CHAT", "gpt-4-turbo-nonp")
OAUTH_URL = os.environ.get("OAUTH_URL", "https://api.example.com/oauth/token")
CLIENT_ID = os.environ.get("OAUTH_CLIENT_ID", "your_client_id")
CLIENT_SECRET = os.environ.get("OAUTH_CLIENT_SECRET", "your_client_secret")
SSL_SOURCE_PATH = os.environ.get("SSL_SOURCE_PATH", "/path/to/your/rbc-ca-bundle.cer")
SSL_LOCAL_PATH = "/tmp/rbc-ca-bundle.cer"

# --- API Parameters ---
MAX_COMPLETION_TOKENS_SECTION = 2000 # Max tokens for section details response
TEMPERATURE = 0.3
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5 # seconds
MAX_RECENT_SUMMARIES_CONTEXT = 5 # Number of previous section summaries to include in context

# --- Token Cost (Optional) ---
PROMPT_TOKEN_COST = 0.01
COMPLETION_TOKEN_COST = 0.03

# --- Section Merging Thresholds (from script 2) ---
# OPTIMAL_TOKENS = 500 # Target token count (currently informational).
MAX_TOKENS = 750  # Maximum tokens allowed in a merged section.
MIN_TOKENS = 250  # Sections below this count trigger merging logic (Pass 1).
ULTRA_SMALL_THRESHOLD = 25  # Sections below this trigger more aggressive merging (Pass 2).


# --- Logging Setup ---
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)
# Use a different log file for this stage
log_file = Path(LOG_DIR) / 'stage2_section_processing.log'
# Remove existing handlers if configuring multiple times in a notebook
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

# Silence overly verbose HTTPX and OpenAI logging
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("openai").setLevel(logging.WARNING)

# ==============================================================================
# Utility Functions (Self-Contained, adapted from Stage 1 and original scripts)
# ==============================================================================

# --- Tokenizer ---
_TOKENIZER = None
if tiktoken:
    try:
        _TOKENIZER = tiktoken.get_encoding("cl100k_base")
        logging.info("Using 'cl100k_base' tokenizer via tiktoken.")
    except Exception as e:
        logging.warning(f"Failed to initialize tiktoken tokenizer: {e}. Falling back to estimate.")
        _TOKENIZER = None

def count_tokens(text: str) -> int:
    """Counts tokens using tiktoken if available, otherwise estimates (chars/4)."""
    if not text: return 0
    if _TOKENIZER:
        try: return len(_TOKENIZER.encode(text))
        except Exception: return len(text) // 4
    else: return len(text) // 4

# --- API Client ---
_SSL_CONFIGURED = False
_OPENAI_CLIENT = None

def _setup_ssl(source_path=SSL_SOURCE_PATH, local_path=SSL_LOCAL_PATH) -> bool:
    """Copies SSL cert locally and sets environment variables."""
    global _SSL_CONFIGURED
    if _SSL_CONFIGURED: return True
    if not Path(source_path).is_file():
         logging.warning(f"SSL source certificate not found at {source_path}. API calls may fail.")
         _SSL_CONFIGURED = True
         return True
    logging.info("Setting up SSL certificate...")
    try:
        source = Path(source_path); local = Path(local_path)
        local.parent.mkdir(parents=True, exist_ok=True)
        with open(source, "rb") as sf, open(local, "wb") as df: df.write(sf.read())
        os.environ["SSL_CERT_FILE"] = str(local)
        os.environ["REQUESTS_CA_BUNDLE"] = str(local)
        logging.info(f"SSL certificate configured successfully at: {local}")
        _SSL_CONFIGURED = True
        return True
    except Exception as e:
        logging.error(f"Error setting up SSL certificate: {e}", exc_info=True)
        return False

def _get_oauth_token(oauth_url=OAUTH_URL, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, ssl_verify_path=SSL_LOCAL_PATH) -> Optional[str]:
    """Retrieves OAuth token."""
    verify_path = ssl_verify_path if Path(ssl_verify_path).exists() else True
    logging.info("Attempting to get OAuth token...")
    payload = {'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret}
    try:
        response = requests.post(oauth_url, data=payload, timeout=30, verify=verify_path)
        response.raise_for_status()
        token_data = response.json(); oauth_token = token_data.get('access_token')
        if not oauth_token: logging.error("Error: 'access_token' not found."); return None
        logging.info("OAuth token obtained successfully.")
        return oauth_token
    except requests.exceptions.RequestException as e:
        logging.error(f"Error getting OAuth token: {e}", exc_info=True); return None

def get_openai_client(base_url=BASE_URL) -> Optional[OpenAI]:
    """Initializes and returns the OpenAI client."""
    global _OPENAI_CLIENT
    if _OPENAI_CLIENT: return _OPENAI_CLIENT
    if not OpenAI: logging.error("OpenAI library not available."); return None
    if not _setup_ssl(): logging.warning("Proceeding without explicit SSL setup.")
    api_key = _get_oauth_token()
    if not api_key: logging.error("Aborting client creation due to OAuth token failure."); return None
    try:
        _OPENAI_CLIENT = OpenAI(api_key=api_key, base_url=base_url)
        logging.info("OpenAI client created successfully.")
        return _OPENAI_CLIENT
    except Exception as e:
        logging.error(f"Error creating OpenAI client: {e}", exc_info=True); return None

# --- API Call (Single Attempt) ---
# Renamed from _call_gpt_with_retry to reflect single attempt nature
def _call_gpt_single_attempt(client, model, messages, max_tokens, temperature, tools=None, tool_choice=None):
    """Makes a single API call attempt."""
    logging.debug("Making single API call attempt...")
    completion_kwargs = {"model": model, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, "stream": False}
    if tools and tool_choice:
        completion_kwargs["tools"] = tools; completion_kwargs["tool_choice"] = tool_choice
        logging.debug("Making API call with tool choice...")
    else:
        # This function now expects tool use based on how get_section_details_from_gpt uses it
        if not tools or not tool_choice:
             logging.warning("API call initiated without explicit tool choice - this function expects tool use.")
        completion_kwargs["tools"] = tools # Still send tools if provided
        # completion_kwargs["response_format"] = {"type": "json_object"} # Use if not using tools

    # No try/except here, let exceptions propagate up to the caller (get_section_details_from_gpt)
    response = client.chat.completions.create(**completion_kwargs)
    logging.debug("API call successful.")
    response_message = response.choices[0].message; usage_info = response.usage

    if response_message.tool_calls:
        tool_call = response_message.tool_calls[0]
        # Basic validation can still happen here or be deferred to parser
        if tool_choice and isinstance(tool_choice, dict):
            expected_tool_name = tool_choice.get("function", {}).get("name")
            if expected_tool_name and tool_call.function.name != expected_tool_name:
                raise ValueError(f"Expected tool '{expected_tool_name}' but received '{tool_call.function.name}'")
        return tool_call.function.arguments, usage_info # Return JSON string from tool arguments
    elif response_message.content:
        # If content is returned instead of tool_calls, return it for the parser to handle
        logging.warning("API response contained content instead of expected tool_calls.")
        return response_message.content, usage_info
    else:
        raise ValueError("API response missing both tool calls and content.")

def parse_gpt_json_response(response_content_str: str, expected_keys: List[str]) -> Optional[Dict]:
    """Parses JSON response string from GPT and validates expected keys. Raises exceptions on failure."""
    # Removed the unnecessary try block
    if response_content_str.strip().startswith("```json"): response_content_str = response_content_str.strip()[7:-3].strip()
    elif response_content_str.strip().startswith("```"): response_content_str = response_content_str.strip()[3:-3].strip()
    data = json.loads(response_content_str)
    # Corrected indentation for the following lines
    if not isinstance(data, dict): raise ValueError("Response is not a JSON object.")
    missing_keys = [key for key in expected_keys if key not in data]
    if missing_keys: raise ValueError(f"Missing expected keys: {', '.join(missing_keys)}")
    logging.debug("GPT JSON response parsed successfully.")
    return data
    # Let exceptions propagate up to the caller (get_section_details_from_gpt)
    # except json.JSONDecodeError as e:
    #     logging.error(f"Error decoding GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None
    # except ValueError as e:
    #     logging.error(f"Error validating GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None

# --- File/Path Utils ---
def create_directory(directory: str):
    """Creates the specified directory if it does not already exist."""
    Path(directory).mkdir(parents=True, exist_ok=True)

# --- Page Tag Extraction (Identical to Stage 1) ---
PAGE_NUMBER_TAG_PATTERN = re.compile(r'<!--\s*PageNumber="(\d+)"\s*-->')
AZURE_TAG_PATTERN = re.compile(r'<!--\s*Page(Footer|Number|Break|Header)=?(".*?"|\d+)?\s*-->\s*\n?')

def clean_azure_tags(text: str) -> str:
    """Removes Azure Document Intelligence specific HTML comment tags."""
    return AZURE_TAG_PATTERN.sub("", text)

def extract_page_mapping(content: str) -> list[tuple[int, int]]:
    """Extracts (character_position, page_number) tuples from tags."""
    mapping = []; raw_matches = []
    for match in PAGE_NUMBER_TAG_PATTERN.finditer(content):
        raw_matches.append((match.start(), int(match.group(1))))
    if not raw_matches: return []
    raw_matches.sort(key=lambda x: (x[0], -x[1]))
    if raw_matches:
        mapping.append(raw_matches[0])
        for i in range(1, len(raw_matches)):
            if raw_matches[i][0] > mapping[-1][0]: mapping.append(raw_matches[i])
    if mapping and mapping[-1][0] < len(content):
        mapping.append((len(content), mapping[-1][1]))
    return mapping

def get_section_page_range(raw_section_slice: str, chapter_start_page: int) -> Tuple[int, int]:
    """Determines start/end page for a section slice."""
    section_mapping = extract_page_mapping(raw_section_slice)
    if not section_mapping:
        # If no tags within the section, assume it's on the chapter's start page
        # This might be inaccurate if sections span pages without internal tags.
        logging.debug("No page tags found within section slice. Using chapter start page.")
        return chapter_start_page, chapter_start_page
    else:
        # Use the first and last page numbers found *within the slice*
        start_page = section_mapping[0][1]
        end_page = section_mapping[-1][1]
        end_page = max(start_page, end_page) # Ensure end >= start
        logging.debug(f"Section page range derived from tags: {start_page}-{end_page}")
        return start_page, end_page

# --- Section Identification ---
def find_headings(raw_content: str) -> list[dict]:
    """Finds Markdown headings (levels 1-6) in raw text."""
    heading_pattern = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)
    headings = []
    for match in heading_pattern.finditer(raw_content):
        headings.append({
            "level": len(match.group(1)),
            "text": match.group(2).strip(),
            "position": match.start()
        })
    # Add virtual end marker
    headings.append({"level": 0, "text": "DOCUMENT_END", "position": len(raw_content)})
    headings.sort(key=lambda h: h["position"])
    return headings

def split_chapter_into_sections(chapter_data: dict) -> list[dict]:
    """Splits raw chapter content into initial sections based on headings."""
    raw_content = chapter_data["raw_content"]
    headings = find_headings(raw_content)
    initial_sections = []
    section_index_in_chapter = 0
    current_heading_context = {f"level_{i}": None for i in range(1, 7)}
    # Initialize L1 with chapter name for context
    current_heading_context["level_1"] = chapter_data.get("chapter_name")

    # Handle content before the first heading
    first_heading_pos = headings[0]['position'] if headings and headings[0]['level'] > 0 else len(raw_content)
    if first_heading_pos > 0:
        intro_slice = raw_content[:first_heading_pos].strip()
        if intro_slice:
            section_index_in_chapter += 1
            initial_sections.append({
                "raw_section_slice": intro_slice,
                "level": 1, # Assign level 1 conceptually
                "section_title": chapter_data.get("chapter_name", "Introduction"), # Use chapter name
                "start_pos": 0,
                "end_pos": first_heading_pos,
                "section_number": section_index_in_chapter,
                "level_1": chapter_data.get("chapter_name"),
            })

    # Process sections defined by headings
    for i in range(len(headings) - 1):
        current_heading = headings[i]
        next_heading = headings[i + 1]

        # Skip if level is 0 (e.g., the intro section we might have handled)
        if current_heading["level"] == 0: continue

        section_start_pos = current_heading["position"]
        section_end_pos = next_heading["position"]
        raw_section_slice = raw_content[section_start_pos:section_end_pos].strip()

        if raw_section_slice: # Only create section if content exists
            section_index_in_chapter += 1
            current_level = current_heading["level"]
            current_title = current_heading["text"]

            # Update heading context
            current_heading_context[f"level_{current_level}"] = current_title
            for lower_level in range(current_level + 1, 7):
                current_heading_context[f"level_{lower_level}"] = None

            section_data = {
                "raw_section_slice": raw_section_slice,
                "level": current_level,
                "section_title": current_title,
                "start_pos": section_start_pos, # Position relative to raw_content start
                "end_pos": section_end_pos,     # Position relative to raw_content start
                "section_number": section_index_in_chapter,
            }
            # Add current hierarchy context
            for level_num in range(1, 7):
                level_key = f"level_{level_num}"
                if current_heading_context.get(level_key):
                    section_data[level_key] = current_heading_context[level_key]

            initial_sections.append(section_data)

    return initial_sections

def generate_hierarchy_string(section_data: dict) -> str:
    """Generates a breadcrumb-style hierarchy string."""
    parts = []
    max_level_to_check = section_data.get("level", 6)
    for i in range(1, max_level_to_check + 1):
        level_key = f"level_{i}"
        heading_text = section_data.get(level_key)
        if heading_text: parts.append(heading_text)
        else: break # Stop if a level is missing
    return " > ".join(parts)

# --- Section Merging Logic (Copied from 1_chunk_creation/2_identify_sections_and_merge.py) ---

def merge_small_sections(
    sections: list[dict], min_tokens: int, max_tokens: int, ultra_small_threshold: int
) -> list[dict]:
    """
    Merges sections smaller than `min_tokens` or `ultra_small_threshold`
    with adjacent sections, respecting hierarchy and `max_tokens` limit.

    Operates in two passes:
    1. Merges sections < `min_tokens` based on level and proximity.
    2. Merges sections < `ultra_small_threshold` more aggressively.

    Args:
        sections: List of cleaned section dictionaries (must include 'content',
                  'section_token_count', 'start_pos', 'end_pos', 'level', 'chapter_number',
                  and other pass-through fields).
        min_tokens: Threshold for the first merging pass.
        max_tokens: Maximum allowed tokens for a merged section.
        ultra_small_threshold: Threshold for the second, more aggressive pass.

    Returns:
        A list of sections after merging potentially small ones.
    """
    if not sections:
        return []

    # Ensure sections are sorted by their original position (section_number) before merging
    sections_to_process = sorted(sections, key=lambda s: s["section_number"])

    # --- Pass 1: Merge sections smaller than `min_tokens` ---
    pass1_merged = []
    i = 0
    while i < len(sections_to_process):
        current = sections_to_process[i]
        # Use section_token_count for merging decisions
        current_tokens = current.get("section_token_count", 0)

        # Keep sections that are already large enough
        if current_tokens >= min_tokens:
            pass1_merged.append(current)
            i += 1
            continue

        # Attempt to merge small sections
        merged_pass1 = False

        # Strategy 1: Merge forward with next section if compatible levels and combined size <= max_tokens
        if i + 1 < len(sections_to_process):
            next_s = sections_to_process[i + 1]
            next_tokens = next_s.get("section_token_count", 0)
            if (
                current["chapter_number"] == next_s["chapter_number"]
                and current.get("level") == next_s.get("level") # Require same level for forward merge
                and current_tokens + next_tokens <= max_tokens
            ):
                # Create merged section data, keeping metadata from the *first* section (current)
                merged_data = current.copy() # Start with current's metadata
                merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                # Recalculate token count for the merged content
                merged_data["section_token_count"] = count_tokens(merged_data["content"])
                # Keep word count for reference, though less critical now (removed word_count field later)
                # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                merged_data["end_pos"] = next_s["end_pos"]
                # All other fields (hierarchy, pass-through) are inherited from 'current'

                pass1_merged.append(merged_data)
                i += 2  # Skip the next section as it's now merged
                merged_pass1 = True

        # Strategy 2: Merge backward with the previously added section if compatible
        if not merged_pass1 and pass1_merged:
            prev_s = pass1_merged[-1]
            prev_tokens = prev_s.get("section_token_count", 0)
            # Check chapter, token limits, and compatible levels (current is same or deeper level)
            if (
                current["chapter_number"] == prev_s["chapter_number"]
                and prev_tokens + current_tokens <= max_tokens
                and current.get("level", 1) >= prev_s.get("level", 1) # Allow merging deeper level back
            ):
                # Merge current's content into the previous section
                prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                # Recalculate token count for the merged content
                prev_s["section_token_count"] = count_tokens(prev_s["content"])
                # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                prev_s["end_pos"] = current["end_pos"]
                # Metadata (hierarchy, pass-through) remains from prev_s

                i += 1 # Move to the next section to process
                merged_pass1 = True

        # If no merge occurred, keep the current section as is
        if not merged_pass1:
            pass1_merged.append(current)
            i += 1

    # --- Pass 2: Merge "ultra-small" sections (< ultra_small_threshold) ---
    if not pass1_merged:  # Skip if Pass 1 resulted in nothing
        return []

    final_merged = []
    i = 0
    while i < len(pass1_merged):
        current = pass1_merged[i]
        # Use section_token_count for decisions here too
        current_tokens = current.get("section_token_count", 0)

        # Keep sections that meet the ultra-small threshold
        if current_tokens >= ultra_small_threshold:
            final_merged.append(current)
            i += 1
            continue

        # Determine if the section content looks like just a heading
        is_heading_only = (
            re.match(r"^\s*#+\s+[^#]", current.get("content", "").strip()) is not None
        )
        merged_pass2 = False

        # --- Preferred Merge Direction ---
        if is_heading_only:
            # Headings prefer merging FORWARD (merge *current* heading into the *next* section's content)
            if i + 1 < len(pass1_merged):
                next_s = pass1_merged[i + 1]
                next_tokens = next_s.get("section_token_count", 0)
                if (
                    current["chapter_number"] == next_s["chapter_number"]
                    and current_tokens + next_tokens <= max_tokens
                ):
                    # Create merged section, taking metadata from NEXT section but content from both
                    merged_data = next_s.copy() # Start with next section's metadata
                    merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                    merged_data["section_token_count"] = count_tokens(merged_data["content"])
                    # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                    merged_data["start_pos"] = current["start_pos"] # Start pos from current
                    # Hierarchy etc. comes from next_s

                    final_merged.append(merged_data)
                    i += 2 # Skip current and next
                    merged_pass2 = True
        else:
            # Content sections prefer merging BACKWARD (merge *current* content into the *previous* section)
            if final_merged:
                prev_s = final_merged[-1]
                prev_tokens = prev_s.get("section_token_count", 0)
                if (
                    current["chapter_number"] == prev_s["chapter_number"]
                    and prev_tokens + current_tokens <= max_tokens
                ):
                    # Merge current's content into previous
                    prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                    prev_s["section_token_count"] = count_tokens(prev_s["content"])
                    # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                    prev_s["end_pos"] = current["end_pos"]
                    # Metadata remains from prev_s
                    i += 1 # Move to next item in pass1_merged
                    merged_pass2 = True

        # --- Fallback Merge Direction (if preferred failed) ---
        if not merged_pass2:
            if is_heading_only:
                # Fallback for heading: Merge BACKWARD (merge *current* heading into *previous*)
                if final_merged:
                    prev_s = final_merged[-1]
                    prev_tokens = prev_s.get("section_token_count", 0)
                    if (
                        current["chapter_number"] == prev_s["chapter_number"]
                        and prev_tokens + current_tokens <= max_tokens
                    ):
                        # Merge current's content into previous
                        prev_s["content"] = f"{prev_s.get('content', '')}\n\n{current.get('content', '')}"
                        prev_s["section_token_count"] = count_tokens(prev_s["content"])
                        # prev_s["word_count"] = prev_s.get("word_count", 0) + current.get("word_count", 0)
                        prev_s["end_pos"] = current["end_pos"]
                        # Metadata remains from prev_s
                        i += 1 # Move to next item in pass1_merged
                        merged_pass2 = True
            else:
                # Fallback for content: Merge FORWARD (merge *current* content into *next*)
                if i + 1 < len(pass1_merged):
                    next_s = pass1_merged[i + 1]
                    next_tokens = next_s.get("section_token_count", 0)
                    if (
                        current["chapter_number"] == next_s["chapter_number"]
                        and current_tokens + next_tokens <= max_tokens
                    ):
                        # Create merged section, taking metadata from NEXT section
                        merged_data = next_s.copy() # Start with next section's metadata
                        merged_data["content"] = f"{current.get('content', '')}\n\n{next_s.get('content', '')}"
                        merged_data["section_token_count"] = count_tokens(merged_data["content"])
                        # merged_data["word_count"] = current.get("word_count", 0) + next_s.get("word_count", 0)
                        merged_data["start_pos"] = current["start_pos"] # Start pos from current
                        # Hierarchy etc. comes from next_s

                        final_merged.append(merged_data)
                        i += 2 # Skip current and next
                        merged_pass2 = True

        # If no merge happened in Pass 2 either, keep the ultra-small section
        if not merged_pass2:
            final_merged.append(current)
            i += 1

    # Rename final token count field for consistency downstream
    for section in final_merged:
        if "section_token_count" in section:
            section["chunk_token_count"] = section.pop("section_token_count")
        # Remove word_count if it exists, as it's not used later
        section.pop("word_count", None)


    return final_merged


# --- GPT Prompting for Section Details ---
SECTION_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "extract_section_details",
        "description": "Extracts detailed information about a specific document section based on its content and the overall chapter context.",
        "parameters": {
            "type": "object",
            "properties": {
                "section_summary": {
                    "type": "string",
                    "description": "A concise summary (1-3 sentences) capturing the core topic or purpose of this section, suitable for reranking search results."
                },
                "section_tags": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of meaningful keywords or tags specific to this section's content, scaled appropriately to the section's length and complexity. These tags will be used as metadata for search reranking."
                },
                "section_standard": {
                    "type": "string",
                    "description": "The primary accounting or reporting standard applicable to this section (e.g., 'IFRS', 'US GAAP', 'N/A')."
                },
                "section_standard_codes": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of specific standard codes explicitly mentioned or directly relevant in the section (e.g., ['IFRS 16', 'IAS 17']). The number of codes should reflect the section's content. These codes will be used as metadata for search reranking."
                },
                "section_importance_score": { # Matches field name in script 8's schema
                    "type": "number",
                    "description": "A score between 0.0 (low importance) and 1.0 (high importance) indicating how crucial this section is to understanding the overall chapter's topic. A score of 0.5 indicates average or unknown importance. This float value will be used for search reranking.",
                    "minimum": 0.0,
                    "maximum": 1.0
                },
                "section_references": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "A list of explicit references to other sections, chapters, or standard codes found within this section's text (e.g., ['See Section 4.5', 'Refer to Chapter 3', 'IAS 36.12']). Provide an empty list [] if none are found. These references provide context."
                }
            },
            # Keep required field name as 'section_importance_score' to match script 8's schema
            "required": ["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
        }
    }
}

def _build_section_prompt(section_text, chapter_summary, chapter_tags, previous_section_summaries=None):
    """Builds the messages list for the section processing call."""
    if previous_section_summaries is None: previous_section_summaries = []

    # System prompt from script 8
    system_prompt = """<role>You are an expert financial reporting specialist.</role>
<source_material>You are analyzing a specific section within a chapter from an EY technical accounting guidance manual. You are provided with the overall chapter summary/tags and summaries of recently processed sections from the same chapter.</source_material>
<task>Your primary task is to generate a **concise summary (1-3 sentences)** for the current section, suitable for use in reranking search results. Additionally, extract relevant tags, the primary applicable accounting standard, and specific standard codes mentioned. Use the 'extract_section_details' tool for your response.</task>
<guardrails>Base your analysis strictly on the provided section text and context. Focus on capturing the core topic/purpose concisely for the summary. Ensure tags and standard codes are precise and derived from the section text.</guardrails>"""

    user_prompt_elements = ["<prompt>"]
    # User prompt elements from script 8
    user_prompt_elements.append("<style>Concise, factual, keyword-focused for summary; technical and precise for other fields.</style>")
    user_prompt_elements.append("<tone>Professional, objective, expert.</tone>")
    user_prompt_elements.append("<audience>Accounting professionals needing specific guidance on this section.</audience>")
    user_prompt_elements.append('<response_format>Use the "extract_section_details" tool.</response_format>')

    user_prompt_elements.append("<overall_chapter_context>")
    user_prompt_elements.append(f"<chapter_summary>{chapter_summary}</chapter_summary>")
    user_prompt_elements.append(f"<chapter_tags>{json.dumps(chapter_tags)}</chapter_tags>")
    user_prompt_elements.append("</overall_chapter_context>")

    if previous_section_summaries:
        user_prompt_elements.append("<recent_section_context>")
        for i, summary in enumerate(previous_section_summaries):
            user_prompt_elements.append(f"<previous_section_{i+1}_summary>{summary}</previous_section_{i+1}_summary>")
        user_prompt_elements.append("</recent_section_context>")

    user_prompt_elements.append(f"<current_section_text>{section_text}</current_section_text>")

    # Instructions block from script 8 (includes 'section_importance' in point 5)
    user_prompt_elements.append("<instructions>")
    user_prompt_elements.append("""
    **Analysis Objective:** Analyze the provided <current_section_text> considering the <overall_chapter_context> and <recent_section_context> (if provided).
    **Action:** Generate the following details for the **current section** using the 'extract_section_details' tool:
    1.  **section_summary:** A **very concise summary (1-3 sentences)** capturing the core topic or purpose of this section. This summary will be used to help rerank search results, so it should be distinct and informative at a glance.
    2.  **section_tags:** Generate a list of meaningful, granular tags specific to THIS SECTION's content. The number of tags should be dynamic and reflect the section's complexity and key topics. These tags are crucial metadata for search reranking.
    3.  **section_standard:** Identify the single, primary accounting standard framework most relevant to THIS SECTION (e.g., 'IFRS', 'US GAAP', 'N/A').
    4.  **section_standard_codes:** List specific standard codes (e.g., 'IFRS 16', 'IAS 36.12', 'ASC 842-10-15') explicitly mentioned or directly and significantly relevant within THIS SECTION's text. The number of codes should be dynamic, reflecting the section's content. Provide an empty list [] if none are applicable. These codes are crucial metadata for search reranking.
    5.  **section_importance_score:** Assign a score between 0.0 (low importance) and 1.0 (high importance) representing how crucial this section's content is for understanding the overall topic of the chapter provided in the <overall_chapter_context>. Consider the section's scope and depth relative to the chapter summary. A score of 0.5 indicates average or unknown importance. Provide a float value (e.g., 0.7). This score will directly influence search result ranking.
    6.  **section_references:** List any explicit textual references made within the <current_section_text> to other sections, chapters, paragraphs, or specific standard codes (e.g., "See Section 4.5", "Refer to Chapter 3", "IAS 36.12"). Provide an empty list [] if no explicit references are found.
    """)
    user_prompt_elements.append("</instructions>")
    user_prompt_elements.append("</prompt>")
    user_prompt = "\n".join(user_prompt_elements)
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    return messages

def get_section_details_from_gpt(section_text: str, chapter_details: Dict, previous_summaries: List[str], client: OpenAI) -> Optional[Dict]:
    """Calls GPT to get structured details for a section."""
    messages = _build_section_prompt(
        section_text=section_text,
        chapter_summary=chapter_details.get('chapter_summary', 'N/A'),
        chapter_tags=chapter_details.get('chapter_tags', []),
        previous_section_summaries=previous_summaries
    )
    prompt_tokens_est = sum(count_tokens(msg["content"]) for msg in messages)
    logging.debug(f"Estimated prompt tokens for section enrichment: {prompt_tokens_est}")

    last_exception = None
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            logging.debug(f"Attempt {attempt + 1}/{API_RETRY_ATTEMPTS} to get and parse details for section...")
            # 1. Call API (single attempt)
            response_content_json_str, usage_info = _call_gpt_single_attempt(
                client, MODEL_NAME_CHAT, messages, MAX_COMPLETION_TOKENS_SECTION, TEMPERATURE,
                tools=[SECTION_TOOL_SCHEMA],
                tool_choice={"type": "function", "function": {"name": "extract_section_details"}}
            )
            if not response_content_json_str:
                raise ValueError("API call returned empty response content.")

            # 2. Parse Response (raises exception on failure)
            parsed_data = parse_gpt_json_response(
                response_content_json_str,
                expected_keys=["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
            )

            # 3. Log usage and return data if successful
            if usage_info:
                 prompt_tokens = usage_info.prompt_tokens; completion_tokens = usage_info.completion_tokens
                 total_tokens = usage_info.total_tokens
                 total_cost = (prompt_tokens / 1000 * PROMPT_TOKEN_COST) + (completion_tokens / 1000 * COMPLETION_TOKEN_COST)
                 # Changed to DEBUG to reduce console noise
                 logging.debug(f"API Usage (Section Details) - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}, Cost: ${total_cost:.4f}")
            else:
                 logging.debug("Usage information not available.")

            return parsed_data # Success! Exit loop and return.

        except (APIError, requests.exceptions.RequestException) as e:
            logging.warning(f"API communication error on attempt {attempt + 1}: {e}")
            last_exception = e
            time.sleep(API_RETRY_DELAY * (attempt + 1)) # Exponential backoff for API errors
        except (json.JSONDecodeError, ValueError) as e:
             logging.warning(f"Parsing/Validation error on attempt {attempt + 1}: {e}")
             # Log raw response if possible (might be large)
             if 'response_content_json_str' in locals() and response_content_json_str:
                 logging.warning(f"Raw response snippet: {response_content_json_str[:500]}...")
             last_exception = e
             time.sleep(API_RETRY_DELAY) # Simple delay for parsing errors
        except Exception as e:
            logging.error(f"Unexpected error on attempt {attempt + 1}: {e}", exc_info=True)
            last_exception = e
            time.sleep(API_RETRY_DELAY) # Simple delay for unexpected errors

    # If loop finishes without returning, all attempts failed
    logging.error(f"Failed to get valid section details after {API_RETRY_ATTEMPTS} attempts.")
    if last_exception:
        logging.error(f"Last error encountered: {last_exception}")
    return None # Indicate failure

import sys # Ensure sys is imported

# ==============================================================================
# Main Stage 2 Logic
# ==============================================================================

# Helper function to create a unique ID for a section
def _create_section_id(section_data: Dict) -> Optional[str]:
    """Creates a unique identifier string for a section."""
    doc_id = section_data.get("document_id")
    chap_num = section_data.get("chapter_number")
    sec_num = section_data.get("section_number")
    if doc_id is not None and chap_num is not None and sec_num is not None:
        return f"{doc_id}::{chap_num}::{sec_num}"
    logging.warning(f"Could not create section ID from data: {section_data.get('section_title', 'Unknown Section')}")
    return None # Return None if essential parts are missing

def process_chapter_for_sections(
    chapter_data: Dict,
    client: Optional[OpenAI],
    existing_section_details: Dict[str, Dict] # Changed: Pass dict of existing details
    ) -> List[Dict]:
    """
    Identifies, enriches, and assembles section data for a single chapter.
    Retries sections if they exist in existing_section_details but lack enrichment.
    Returns a list of all section data dictionaries for this chapter (new, retried, or existing valid).
    """
    chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
    document_id = chapter_data.get("document_id", "UNKNOWN_DOC") # Get doc_id for ID creation
    logging.info(f"Processing sections for Chapter {chapter_number}...")
    processed_chapter_sections = [] # Store all sections for this chapter
    recent_section_summaries = [] # Context for GPT within this chapter run

    # 1. Initial split based on headings
    initial_sections = split_chapter_into_sections(chapter_data)
    logging.info(f"  Identified {len(initial_sections)} initial sections.")

    # 2. Clean content and calculate initial metrics
    cleaned_sections = []
    for section_raw in initial_sections:
        # Clean Azure tags from the raw slice
        cleaned_content = clean_azure_tags(section_raw["raw_section_slice"])
        # Only keep sections with non-whitespace content after cleaning
        if cleaned_content.strip():
            section_clean = section_raw.copy() # Keep all fields from initial split
            section_clean["content"] = cleaned_content # Store cleaned content
            # Calculate and store the *initial* token count needed for merging
            section_clean["section_token_count"] = count_tokens(cleaned_content)
            # Remove raw slice now that we have cleaned content
            section_clean.pop("raw_section_slice", None)
            cleaned_sections.append(section_clean)
    logging.info(f"  Sections after cleaning & filtering empty: {len(cleaned_sections)}")

    # 3. Merge small sections
    merged_sections = merge_small_sections(
        cleaned_sections, MIN_TOKENS, MAX_TOKENS, ULTRA_SMALL_THRESHOLD
    )
    logging.info(f"  Sections after merging small ones: {len(merged_sections)}")


    # 4. Process merged sections (enrichment, resumability check)
    skipped_fully_processed_count = 0
    retried_count = 0
    newly_processed_count = 0
    failed_processing_count = 0

    # Iterate through the *merged* sections now
    for section_data in tqdm(merged_sections, desc=f"Chapter {chapter_number} Sections"):
        # section_data now contains 'content', 'section_token_count' (renamed to chunk_token_count by merge func),
        # 'level', 'section_title', 'start_pos', 'end_pos', 'section_number', level_X fields, etc.

        section_number = section_data["section_number"] # Use the number from the (potentially merged) section
        section_title = section_data.get('section_title', 'Unknown Title')

        # Create ID for checking/storing using potentially merged section data
        temp_id_data = {"document_id": document_id, "chapter_number": chapter_number, "section_number": section_number}
        section_id = _create_section_id(temp_id_data)
        if not section_id:
            logging.warning(f"Could not generate ID for section {section_number} ('{section_title[:30]}...'). Skipping.")
            failed_processing_count += 1
            continue

        # --- Check existing data ---
        existing_record = existing_section_details.get(section_id)
        needs_enrichment = True
        if existing_record:
            # Check if enrichment exists (using section_summary as a proxy)
            if existing_record.get("section_summary") is not None:
                logging.debug(f"  Section {section_id} already processed with enrichment. Using existing.")
                # Use the existing enriched record directly
                final_section_data = existing_record
                processed_chapter_sections.append(final_section_data)
                # Update context window even if skipped
                if final_section_data.get("section_summary"):
                    recent_section_summaries.append(final_section_data["section_summary"])
                skipped_fully_processed_count += 1
                needs_enrichment = False
            else:
                # Found existing record but it lacks enrichment, needs processing
                logging.debug(f"  Section {section_id} found with missing enrichment. Retrying enrichment.")
                retried_count += 1
        else:
             # Section not found in existing data, needs processing
             logging.debug(f"  Section {section_id} not found in existing data. Processing as new.")
             newly_processed_count += 1

        if not needs_enrichment:
            continue # Go to next section in merged_sections

        # --- Enrich Section (New or Retry) ---
        logging.debug(f"  Enriching Section {section_id} ('{section_title[:30]}...')")

        # Content for enrichment comes from the potentially merged section_data
        content_for_gpt = section_data.get("content", "")
        if not content_for_gpt:
             logging.warning(f"  Skipping enrichment for section {section_id} due to empty content after merge.")
             # Still need to assemble basic data for this section
             gpt_details = None
        elif client:
            # Use only the last N summaries for context
            context_summaries = recent_section_summaries[-MAX_RECENT_SUMMARIES_CONTEXT:]
            # Pass potentially merged content to GPT
            gpt_details = get_section_details_from_gpt(content_for_gpt, chapter_data, context_summaries, client)
        else:
            logging.debug("  Skipping GPT enrichment (no client).")
            gpt_details = None

        # Assemble final section data using the merged section_data as base
        # and adding GPT details if available
        final_section_data = section_data.copy() # Start with merged section data

        # Add/update fields based on GPT enrichment or defaults
        final_section_data["section_summary"] = gpt_details.get("section_summary", None) if gpt_details else None
        final_section_data["section_tags"] = gpt_details.get("section_tags", []) if gpt_details else []
        final_section_data["section_standard"] = gpt_details.get("section_standard", "N/A") if gpt_details else "N/A"
        final_section_data["section_standard_codes"] = gpt_details.get("section_standard_codes", []) if gpt_details else []
        final_section_data["section_importance_score"] = gpt_details.get("section_importance_score", 0.5) if gpt_details else 0.5
        final_section_data["section_references"] = gpt_details.get("section_references", []) if gpt_details else []

        # Ensure required fields from merge/split are present
        final_section_data["document_id"] = chapter_data.get("document_id")
        final_section_data["chapter_number"] = chapter_number
        final_section_data["chapter_name"] = chapter_data.get("chapter_name")
        final_section_data["chapter_tags"] = chapter_data.get("chapter_tags") # From stage 1
        final_section_data["chapter_summary"] = chapter_data.get("chapter_summary") # From stage 1
        final_section_data["chapter_token_count"] = chapter_data.get("chapter_token_count") # From stage 1

        # Rename 'content' to 'cleaned_section_content' for final output consistency
        if "content" in final_section_data:
            final_section_data["cleaned_section_content"] = final_section_data.pop("content")

        # Ensure hierarchy string is present (it should be from merge_small_sections inheriting it)
        if "section_hierarchy" not in final_section_data:
             final_section_data["section_hierarchy"] = generate_hierarchy_string(final_section_data)

        # Add page numbers (needs logic if merge affects this - assume start/end from original section for now)
        # TODO: Revisit page number logic if merging significantly changes span.
        # For now, use chapter pages as fallback if section pages are missing after merge.
        final_section_data["section_start_page"] = section_data.get("section_start_page", chapter_data.get("chapter_page_start"))
        final_section_data["section_end_page"] = section_data.get("section_end_page", chapter_data.get("chapter_page_end"))

        # Ensure token count field name is consistent ('chunk_token_count' was set by merge func)
        if "chunk_token_count" not in final_section_data:
             final_section_data["chunk_token_count"] = count_tokens(final_section_data.get("cleaned_section_content", ""))


        processed_chapter_sections.append(final_section_data) # Add the processed data

        # Update recent summaries list for context (use summary from final_section_data)
        current_summary = final_section_data.get("section_summary")
        if current_summary: # Only add if enrichment was successful (not None)
            recent_section_summaries.append(current_summary)
            # Keep only the last N summaries
            if len(recent_section_summaries) > MAX_RECENT_SUMMARIES_CONTEXT:
                recent_section_summaries.pop(0)
        elif gpt_details is None: # Log if enrichment failed
             failed_processing_count += 1
             logging.warning(f"  Failed to get enrichment for section {section_id} after retries.")


    # Log chapter summary
    logging.info(f"  Chapter {chapter_number} Summary: Skipped={skipped_fully_processed_count}, Retried={retried_count}, New={newly_processed_count}, Failed={failed_processing_count}")
    return processed_chapter_sections # Return all sections processed/retrieved for this chapter


def run_stage2():
    """Main function to execute Stage 2 processing with resumability."""
    logging.info("--- Starting Stage 2: Section Identification & Enrichment ---")
    create_directory(OUTPUT_DIR)
    output_filepath = Path(OUTPUT_DIR) / OUTPUT_FILENAME

    # --- Load Existing Stage 2 Data (for Resumability) ---
    existing_section_details = {} # Store as dict: {section_id: section_data}
    if output_filepath.exists():
        try:
            with open(output_filepath, "r", encoding="utf-8") as f:
                existing_data_list = json.load(f)
            if not isinstance(existing_data_list, list):
                 logging.warning(f"Existing output file {output_filepath} does not contain a valid list. Starting fresh.")
            else:
                 # Populate the dictionary, creating IDs
                 count = 0
                 for sec_data in existing_data_list:
                     sec_id = _create_section_id(sec_data)
                     if sec_id:
                         existing_section_details[sec_id] = sec_data
                         count += 1
                 logging.info(f"Loaded {count} existing section records into map from {output_filepath}.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding JSON from {output_filepath}. Starting fresh.", exc_info=True)
            existing_section_details = {}
        except Exception as e:
            logging.error(f"Error loading existing data from {output_filepath}: {e}. Starting fresh.", exc_info=True)
            existing_section_details = {}

    # --- Load Stage 1 Data ---
    stage1_output_file = Path(STAGE1_OUTPUT_DIR) / STAGE1_FILENAME
    if not stage1_output_file.exists():
        logging.error(f"Stage 1 output file not found: {stage1_output_file}. Cannot proceed.")
        # Exit if stage 1 data is missing
        sys.exit(f"Error: Stage 1 output file '{stage1_output_file}' not found.")
    try:
        with open(stage1_output_file, "r", encoding="utf-8") as f:
            all_chapter_data = json.load(f)
        logging.info(f"Loaded {len(all_chapter_data)} chapters from {stage1_output_file}")
    except Exception as e:
        logging.error(f"Error loading Stage 1 data from {stage1_output_file}: {e}", exc_info=True)
        # Exit if stage 1 data is unloadable
        sys.exit(f"Error: Failed to load Stage 1 data from '{stage1_output_file}'.")

    if not all_chapter_data:
        logging.warning("Stage 1 data is empty. No sections to process.")
        # Save the potentially loaded (but empty) existing data back
        try:
            with open(output_filepath, "w", encoding="utf-8") as f:
                # Save values from the map as a list
                json.dump(list(existing_section_details.values()), f, indent=2, ensure_ascii=False)
            logging.info(f"No chapters to process. Saved {len(existing_section_details)} existing section records back to {output_filepath}")
        except Exception as e:
            logging.error(f"Error saving output JSON to {output_filepath}: {e}", exc_info=True)
        return list(existing_section_details.values()) # Return list

    # --- Initialize OpenAI Client ---
    client = get_openai_client()
    if not client:
        logging.warning("OpenAI client initialization failed. Section enrichment will be skipped.")

    # --- Process Chapters for Sections ---
    all_processed_sections_list = [] # Collect results from all chapters here
    total_sections_processed_run = 0
    total_sections_failed_run = 0
    total_sections_skipped_run = 0

    for chapter_data in tqdm(all_chapter_data, desc="Processing Chapters for Sections"):
        # Pass the dictionary of existing details for checking/retrying
        chapter_results = process_chapter_for_sections(
            chapter_data, client, existing_section_details
        )
        all_processed_sections_list.extend(chapter_results)

        # Optional: Update counts based on logs/return values if needed for final summary
        # This requires process_chapter_for_sections to return counts or parse logs

    # --- Final Sort and Save ---
    logging.info("Performing final sort and save...")
    final_data_to_save = all_processed_sections_list # Already a list
    if natsort:
        try:
            # Sort by chapter then section number
            final_data_to_save = sorted(all_processed_sections_list, key=lambda x: (
                x.get('chapter_number', float('inf')),
                x.get('section_number', float('inf'))
            ))
            logging.info("Performed final sort of section data.")
        except Exception as final_sort_e:
            logging.warning(f"Could not perform final sort: {final_sort_e}. Saving potentially unsorted data.")
            # Keep final_data_to_save as the unsorted list
    else:
        logging.info("natsort not available, skipping final sort.")

    try:
        with open(output_filepath, "w", encoding="utf-8") as f:
            json.dump(final_data_to_save, f, indent=2, ensure_ascii=False)
        logging.info(f"Saved final data with {len(final_data_to_save)} section records to {output_filepath}")
    except Exception as e:
        logging.error(f"Error saving final output JSON to {output_filepath}: {e}", exc_info=True)


    # --- Print Summary ---
    final_record_count = len(final_data_to_save)
    # TODO: Enhance summary by tracking processed/failed/skipped counts more accurately if needed
    logging.info("--- Stage 2 Summary ---")
    logging.info(f"Total chapters from Stage 1: {len(all_chapter_data)}")
    logging.info(f"Total sections in final file : {final_record_count}")
    # Add more detailed counts here if implemented
    logging.info(f"Output JSON file             : {output_filepath}")
    logging.info("--- Stage 2 Finished ---")

    return final_data_to_save # Return the final data list

# ==============================================================================
# Main Execution Block
# ==============================================================================

if __name__ == "__main__":
    run_stage2()
