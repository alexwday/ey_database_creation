# -*- coding: utf-8 -*-
"""
Stage 2: Section Identification & Enrichment

Purpose:
Processes chapter data generated by Stage 1. It identifies logical sections
within each chapter based on Markdown headings (H1-H6). For each section, it
calculates token count, determines start/end page numbers, and uses an LLM
to generate a concise summary, tags, applicable standard(s), standard codes,
an importance score, and references.

Input: JSON file from Stage 1 (e.g., 'pipeline_output/stage1/stage1_chapter_data.json').
Output: A JSON file in OUTPUT_DIR containing a list of section dictionaries.
        (e.g., 'pipeline_output/stage2/stage2_section_data.json')
"""

import os
import json
import traceback
import re
import time
import logging
import requests
from pathlib import Path
from typing import List, Dict, Tuple, Any, Optional, Union
from collections import defaultdict

# --- Dependencies Check ---
try:
    import tiktoken
except ImportError:
    tiktoken = None
    print("WARNING: tiktoken not installed. Token counts will be estimated (chars/4). `pip install tiktoken`")

try:
    import natsort
except ImportError:
    natsort = None
    print("INFO: natsort not installed. Chapters/Sections might not sort naturally. `pip install natsort`")

try:
    from openai import OpenAI, APIError
except ImportError:
    OpenAI = None
    APIError = None
    print("ERROR: openai library not installed. GPT features unavailable. `pip install openai`")

try:
    from tqdm import tqdm
except ImportError:
    tqdm = lambda x, **kwargs: x # Make tqdm optional
    print("INFO: tqdm not installed. Progress bars disabled. `pip install tqdm`")

# ==============================================================================
# Configuration
# ==============================================================================

# --- Directory Paths ---
# TODO: Adjust these paths as needed
STAGE1_OUTPUT_DIR = "pipeline_output/stage1"
STAGE1_FILENAME = "stage1_chapter_data.json"
OUTPUT_DIR = "pipeline_output/stage2"
OUTPUT_FILENAME = "stage2_section_data.json"
LOG_DIR = "pipeline_output/logs"

# --- API Configuration ---
# TODO: Load securely or replace placeholders (Ensure consistency with Stage 1)
BASE_URL = os.environ.get("OPENAI_API_BASE", "https://api.example.com/v1")
MODEL_NAME_CHAT = os.environ.get("OPENAI_MODEL_CHAT", "gpt-4-turbo-nonp")
OAUTH_URL = os.environ.get("OAUTH_URL", "https://api.example.com/oauth/token")
CLIENT_ID = os.environ.get("OAUTH_CLIENT_ID", "your_client_id")
CLIENT_SECRET = os.environ.get("OAUTH_CLIENT_SECRET", "your_client_secret")
SSL_SOURCE_PATH = os.environ.get("SSL_SOURCE_PATH", "/path/to/your/rbc-ca-bundle.cer")
SSL_LOCAL_PATH = "/tmp/rbc-ca-bundle.cer"

# --- API Parameters ---
MAX_COMPLETION_TOKENS_SECTION = 2000 # Max tokens for section details response
TEMPERATURE = 0.3
API_RETRY_ATTEMPTS = 3
API_RETRY_DELAY = 5 # seconds
MAX_RECENT_SUMMARIES_CONTEXT = 5 # Number of previous section summaries to include in context

# --- Token Cost (Optional) ---
PROMPT_TOKEN_COST = 0.01
COMPLETION_TOKEN_COST = 0.03

# --- Logging Setup ---
Path(LOG_DIR).mkdir(parents=True, exist_ok=True)
# Use a different log file for this stage
log_file = Path(LOG_DIR) / 'stage2_section_processing.log'
# Remove existing handlers if configuring multiple times in a notebook
for handler in logging.root.handlers[:]:
    logging.root.removeHandler(handler)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - [%(filename)s:%(lineno)d] - %(message)s',
    handlers=[
        logging.FileHandler(log_file),
        logging.StreamHandler()
    ]
)

# ==============================================================================
# Utility Functions (Self-Contained, adapted from Stage 1 and original scripts)
# ==============================================================================

# --- Tokenizer ---
_TOKENIZER = None
if tiktoken:
    try:
        _TOKENIZER = tiktoken.get_encoding("cl100k_base")
        logging.info("Using 'cl100k_base' tokenizer via tiktoken.")
    except Exception as e:
        logging.warning(f"Failed to initialize tiktoken tokenizer: {e}. Falling back to estimate.")
        _TOKENIZER = None

def count_tokens(text: str) -> int:
    """Counts tokens using tiktoken if available, otherwise estimates (chars/4)."""
    if not text: return 0
    if _TOKENIZER:
        try: return len(_TOKENIZER.encode(text))
        except Exception: return len(text) // 4
    else: return len(text) // 4

# --- API Client ---
_SSL_CONFIGURED = False
_OPENAI_CLIENT = None

def _setup_ssl(source_path=SSL_SOURCE_PATH, local_path=SSL_LOCAL_PATH) -> bool:
    """Copies SSL cert locally and sets environment variables."""
    global _SSL_CONFIGURED
    if _SSL_CONFIGURED: return True
    if not Path(source_path).is_file():
         logging.warning(f"SSL source certificate not found at {source_path}. API calls may fail.")
         _SSL_CONFIGURED = True
         return True
    logging.info("Setting up SSL certificate...")
    try:
        source = Path(source_path); local = Path(local_path)
        local.parent.mkdir(parents=True, exist_ok=True)
        with open(source, "rb") as sf, open(local, "wb") as df: df.write(sf.read())
        os.environ["SSL_CERT_FILE"] = str(local)
        os.environ["REQUESTS_CA_BUNDLE"] = str(local)
        logging.info(f"SSL certificate configured successfully at: {local}")
        _SSL_CONFIGURED = True
        return True
    except Exception as e:
        logging.error(f"Error setting up SSL certificate: {e}", exc_info=True)
        return False

def _get_oauth_token(oauth_url=OAUTH_URL, client_id=CLIENT_ID, client_secret=CLIENT_SECRET, ssl_verify_path=SSL_LOCAL_PATH) -> Optional[str]:
    """Retrieves OAuth token."""
    verify_path = ssl_verify_path if Path(ssl_verify_path).exists() else True
    logging.info("Attempting to get OAuth token...")
    payload = {'grant_type': 'client_credentials', 'client_id': client_id, 'client_secret': client_secret}
    try:
        response = requests.post(oauth_url, data=payload, timeout=30, verify=verify_path)
        response.raise_for_status()
        token_data = response.json(); oauth_token = token_data.get('access_token')
        if not oauth_token: logging.error("Error: 'access_token' not found."); return None
        logging.info("OAuth token obtained successfully.")
        return oauth_token
    except requests.exceptions.RequestException as e:
        logging.error(f"Error getting OAuth token: {e}", exc_info=True); return None

def get_openai_client(base_url=BASE_URL) -> Optional[OpenAI]:
    """Initializes and returns the OpenAI client."""
    global _OPENAI_CLIENT
    if _OPENAI_CLIENT: return _OPENAI_CLIENT
    if not OpenAI: logging.error("OpenAI library not available."); return None
    if not _setup_ssl(): logging.warning("Proceeding without explicit SSL setup.")
    api_key = _get_oauth_token()
    if not api_key: logging.error("Aborting client creation due to OAuth token failure."); return None
    try:
        _OPENAI_CLIENT = OpenAI(api_key=api_key, base_url=base_url)
        logging.info("OpenAI client created successfully.")
        return _OPENAI_CLIENT
    except Exception as e:
        logging.error(f"Error creating OpenAI client: {e}", exc_info=True); return None

# --- API Call ---
def _call_gpt_with_retry(client, model, messages, max_tokens, temperature, tools=None, tool_choice=None):
    """Makes the API call with retry logic."""
    last_exception = None
    for attempt in range(API_RETRY_ATTEMPTS):
        try:
            logging.debug(f"Making API call (Attempt {attempt + 1}/{API_RETRY_ATTEMPTS})...")
            completion_kwargs = {"model": model, "messages": messages, "max_tokens": max_tokens, "temperature": temperature, "stream": False}
            if tools and tool_choice:
                completion_kwargs["tools"] = tools; completion_kwargs["tool_choice"] = tool_choice
                logging.debug("Making API call with tool choice...")
            else: completion_kwargs["response_format"] = {"type": "json_object"}; logging.debug("Making API call with JSON response format...")

            response = client.chat.completions.create(**completion_kwargs)
            logging.debug("API call successful.")
            response_message = response.choices[0].message; usage_info = response.usage

            if response_message.tool_calls:
                tool_call = response_message.tool_calls[0]
                if tool_choice and isinstance(tool_choice, dict):
                    expected_tool_name = tool_choice.get("function", {}).get("name")
                    if expected_tool_name and tool_call.function.name != expected_tool_name: raise ValueError(f"Expected tool '{expected_tool_name}' but received '{tool_call.function.name}'")
                return tool_call.function.arguments, usage_info
            elif response_message.content: return response_message.content, usage_info
            else: raise ValueError("API response missing both tool calls and content.")
        except APIError as e:
            logging.warning(f"API Error on attempt {attempt + 1}: {e}"); last_exception = e; time.sleep(API_RETRY_DELAY * (attempt + 1))
        except Exception as e:
            logging.warning(f"Non-API Error on attempt {attempt + 1}: {e}", exc_info=True); last_exception = e; time.sleep(API_RETRY_DELAY)
    logging.error(f"API call failed after {API_RETRY_ATTEMPTS} attempts.")
    if last_exception: raise last_exception
    else: raise Exception("API call failed for unknown reasons.")

def parse_gpt_json_response(response_content_str: str, expected_keys: List[str]) -> Optional[Dict]:
    """Parses JSON response string from GPT and validates expected keys."""
    try:
        if response_content_str.strip().startswith("```json"): response_content_str = response_content_str.strip()[7:-3].strip()
        elif response_content_str.strip().startswith("```"): response_content_str = response_content_str.strip()[3:-3].strip()
        data = json.loads(response_content_str)
        if not isinstance(data, dict): raise ValueError("Response is not a JSON object.")
        missing_keys = [key for key in expected_keys if key not in data]
        if missing_keys: raise ValueError(f"Missing expected keys: {', '.join(missing_keys)}")
        logging.debug("GPT JSON response parsed successfully.")
        return data
    except json.JSONDecodeError as e:
        logging.error(f"Error decoding GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None
    except ValueError as e:
        logging.error(f"Error validating GPT JSON: {e}\nRaw response: {response_content_str[:500]}..."); return None

# --- File/Path Utils ---
def create_directory(directory: str):
    """Creates the specified directory if it does not already exist."""
    Path(directory).mkdir(parents=True, exist_ok=True)

# --- Page Tag Extraction (Identical to Stage 1) ---
PAGE_NUMBER_TAG_PATTERN = re.compile(r'<!--\s*PageNumber="(\d+)"\s*-->')
AZURE_TAG_PATTERN = re.compile(r'<!--\s*Page(Footer|Number|Break|Header)=?(".*?"|\d+)?\s*-->\s*\n?')

def clean_azure_tags(text: str) -> str:
    """Removes Azure Document Intelligence specific HTML comment tags."""
    return AZURE_TAG_PATTERN.sub("", text)

def extract_page_mapping(content: str) -> list[tuple[int, int]]:
    """Extracts (character_position, page_number) tuples from tags."""
    mapping = []; raw_matches = []
    for match in PAGE_NUMBER_TAG_PATTERN.finditer(content):
        raw_matches.append((match.start(), int(match.group(1))))
    if not raw_matches: return []
    raw_matches.sort(key=lambda x: (x[0], -x[1]))
    if raw_matches:
        mapping.append(raw_matches[0])
        for i in range(1, len(raw_matches)):
            if raw_matches[i][0] > mapping[-1][0]: mapping.append(raw_matches[i])
    if mapping and mapping[-1][0] < len(content):
        mapping.append((len(content), mapping[-1][1]))
    return mapping

def get_section_page_range(raw_section_slice: str, chapter_start_page: int) -> Tuple[int, int]:
    """Determines start/end page for a section slice."""
    section_mapping = extract_page_mapping(raw_section_slice)
    if not section_mapping:
        # If no tags within the section, assume it's on the chapter's start page
        # This might be inaccurate if sections span pages without internal tags.
        logging.debug("No page tags found within section slice. Using chapter start page.")
        return chapter_start_page, chapter_start_page
    else:
        # Use the first and last page numbers found *within the slice*
        start_page = section_mapping[0][1]
        end_page = section_mapping[-1][1]
        end_page = max(start_page, end_page) # Ensure end >= start
        logging.debug(f"Section page range derived from tags: {start_page}-{end_page}")
        return start_page, end_page

# --- Section Identification ---
def find_headings(raw_content: str) -> list[dict]:
    """Finds Markdown headings (levels 1-6) in raw text."""
    heading_pattern = re.compile(r"^(#{1,6})\s+(.+)$", re.MULTILINE)
    headings = []
    for match in heading_pattern.finditer(raw_content):
        headings.append({
            "level": len(match.group(1)),
            "text": match.group(2).strip(),
            "position": match.start()
        })
    # Add virtual end marker
    headings.append({"level": 0, "text": "DOCUMENT_END", "position": len(raw_content)})
    headings.sort(key=lambda h: h["position"])
    return headings

def split_chapter_into_sections(chapter_data: dict) -> list[dict]:
    """Splits raw chapter content into initial sections based on headings."""
    raw_content = chapter_data["raw_content"]
    headings = find_headings(raw_content)
    initial_sections = []
    section_index_in_chapter = 0
    current_heading_context = {f"level_{i}": None for i in range(1, 7)}
    # Initialize L1 with chapter name for context
    current_heading_context["level_1"] = chapter_data.get("chapter_name")

    # Handle content before the first heading
    first_heading_pos = headings[0]['position'] if headings and headings[0]['level'] > 0 else len(raw_content)
    if first_heading_pos > 0:
        intro_slice = raw_content[:first_heading_pos].strip()
        if intro_slice:
            section_index_in_chapter += 1
            initial_sections.append({
                "raw_section_slice": intro_slice,
                "level": 1, # Assign level 1 conceptually
                "section_title": chapter_data.get("chapter_name", "Introduction"), # Use chapter name
                "start_pos": 0,
                "end_pos": first_heading_pos,
                "section_number": section_index_in_chapter,
                "level_1": chapter_data.get("chapter_name"),
            })

    # Process sections defined by headings
    for i in range(len(headings) - 1):
        current_heading = headings[i]
        next_heading = headings[i + 1]

        # Skip if level is 0 (e.g., the intro section we might have handled)
        if current_heading["level"] == 0: continue

        section_start_pos = current_heading["position"]
        section_end_pos = next_heading["position"]
        raw_section_slice = raw_content[section_start_pos:section_end_pos].strip()

        if raw_section_slice: # Only create section if content exists
            section_index_in_chapter += 1
            current_level = current_heading["level"]
            current_title = current_heading["text"]

            # Update heading context
            current_heading_context[f"level_{current_level}"] = current_title
            for lower_level in range(current_level + 1, 7):
                current_heading_context[f"level_{lower_level}"] = None

            section_data = {
                "raw_section_slice": raw_section_slice,
                "level": current_level,
                "section_title": current_title,
                "start_pos": section_start_pos, # Position relative to raw_content start
                "end_pos": section_end_pos,     # Position relative to raw_content start
                "section_number": section_index_in_chapter,
            }
            # Add current hierarchy context
            for level_num in range(1, 7):
                level_key = f"level_{level_num}"
                if current_heading_context.get(level_key):
                    section_data[level_key] = current_heading_context[level_key]

            initial_sections.append(section_data)

    return initial_sections

def generate_hierarchy_string(section_data: dict) -> str:
    """Generates a breadcrumb-style hierarchy string."""
    parts = []
    max_level_to_check = section_data.get("level", 6)
    for i in range(1, max_level_to_check + 1):
        level_key = f"level_{i}"
        heading_text = section_data.get(level_key)
        if heading_text: parts.append(heading_text)
        else: break # Stop if a level is missing
    return " > ".join(parts)

# --- GPT Prompting for Section Details ---
SECTION_TOOL_SCHEMA = {
    "type": "function",
    "function": {
        "name": "extract_section_details",
        "description": "Extracts detailed information about a specific document section based on its content and the overall chapter context.",
        "parameters": {
            "type": "object",
            "properties": {
                "section_summary": {"type": "string", "description": "A concise summary (1-3 sentences) capturing the core topic or purpose of this section."},
                "section_tags": {"type": "array", "items": {"type": "string"}, "description": "Meaningful keywords or tags specific to this section's content."},
                "section_standard": {"type": "string", "description": "The primary accounting or reporting standard applicable (e.g., 'IFRS', 'US GAAP', 'N/A')."},
                "section_standard_codes": {"type": "array", "items": {"type": "string"}, "description": "Specific standard codes explicitly mentioned or directly relevant (e.g., ['IFRS 16', 'IAS 17'])."},
                "section_importance_score": {"type": "number", "description": "Score 0.0-1.0 indicating section importance relative to the chapter. 0.5 is average/unknown.", "minimum": 0.0, "maximum": 1.0},
                "section_references": {"type": "array", "items": {"type": "string"}, "description": "Explicit references to other sections/chapters/standards found in the text (e.g., ['See Section 4.5', 'IAS 36.12']). Empty list [] if none."}
            },
            "required": ["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
        }
    }
}

def _build_section_prompt(section_text, chapter_summary, chapter_tags, previous_section_summaries=None):
    """Builds the messages list for the section processing call."""
    if previous_section_summaries is None: previous_section_summaries = []
    system_prompt = "<role>You are an expert financial reporting specialist.</role><task>Analyze the provided section text within the chapter context. Generate a concise summary, tags, standard info, importance score, and references using the 'extract_section_details' tool.</task><guardrails>Focus on the current section. Base details strictly on provided text and context.</guardrails>"
    user_prompt_elements = ["<prompt>"]
    user_prompt_elements.append("<style>Concise, factual, keyword-focused summary; technical/precise elsewhere.</style>")
    user_prompt_elements.append('<response_format>Use the "extract_section_details" tool.</response_format>')
    user_prompt_elements.append("<overall_chapter_context>")
    user_prompt_elements.append(f"<chapter_summary>{chapter_summary}</chapter_summary>")
    user_prompt_elements.append(f"<chapter_tags>{json.dumps(chapter_tags)}</chapter_tags>")
    user_prompt_elements.append("</overall_chapter_context>")
    if previous_section_summaries:
        user_prompt_elements.append("<recent_section_context>")
        for i, summary in enumerate(previous_section_summaries): user_prompt_elements.append(f"<previous_section_{i+1}_summary>{summary}</previous_section_{i+1}_summary>")
        user_prompt_elements.append("</recent_section_context>")
    user_prompt_elements.append(f"<current_section_text>{section_text}</current_section_text>")
    user_prompt_elements.append("<instructions>")
    user_prompt_elements.append("Analyze <current_section_text> considering context. Use 'extract_section_details' tool for: 1. section_summary (1-3 sentences, core topic). 2. section_tags (specific keywords). 3. section_standard ('IFRS', 'US GAAP', 'N/A'). 4. section_standard_codes (explicit codes like 'IFRS 16'). 5. section_importance_score (0.0-1.0 float, importance to chapter). 6. section_references (explicit refs like 'See Sec 4.5').")
    user_prompt_elements.append("</instructions>")
    user_prompt_elements.append("</prompt>")
    user_prompt = "\n".join(user_prompt_elements)
    messages = [{"role": "system", "content": system_prompt}, {"role": "user", "content": user_prompt}]
    return messages

def get_section_details_from_gpt(section_text: str, chapter_details: Dict, previous_summaries: List[str], client: OpenAI) -> Optional[Dict]:
    """Calls GPT to get structured details for a section."""
    messages = _build_section_prompt(
        section_text=section_text,
        chapter_summary=chapter_details.get('chapter_summary', 'N/A'),
        chapter_tags=chapter_details.get('chapter_tags', []),
        previous_section_summaries=previous_summaries
    )
    prompt_tokens_est = sum(count_tokens(msg["content"]) for msg in messages)
    logging.debug(f"Estimated prompt tokens for section enrichment: {prompt_tokens_est}")

    try:
        response_content_json_str, usage_info = _call_gpt_with_retry(
            client, MODEL_NAME_CHAT, messages, MAX_COMPLETION_TOKENS_SECTION, TEMPERATURE,
            tools=[SECTION_TOOL_SCHEMA],
            tool_choice={"type": "function", "function": {"name": "extract_section_details"}}
        )
        if not response_content_json_str: raise ValueError("API call returned empty response.")

        parsed_data = parse_gpt_json_response(
            response_content_json_str,
            expected_keys=["section_summary", "section_tags", "section_standard", "section_standard_codes", "section_importance_score", "section_references"]
        )

        if usage_info:
             prompt_tokens = usage_info.prompt_tokens; completion_tokens = usage_info.completion_tokens
             total_tokens = usage_info.total_tokens
             total_cost = (prompt_tokens / 1000 * PROMPT_TOKEN_COST) + (completion_tokens / 1000 * COMPLETION_TOKEN_COST)
             logging.info(f"API Usage (Section Details) - Prompt: {prompt_tokens}, Completion: {completion_tokens}, Total: {total_tokens}, Cost: ${total_cost:.4f}")
        else: logging.debug("Usage information not available.")

        return parsed_data

    except Exception as e:
        logging.error(f"Error getting section details from GPT: {e}", exc_info=True)
        return None

import sys # Ensure sys is imported

# ==============================================================================
# Main Stage 2 Logic
# ==============================================================================

# Helper function to create a unique ID for a section
def _create_section_id(section_data: Dict) -> Optional[str]:
    """Creates a unique identifier string for a section."""
    doc_id = section_data.get("document_id")
    chap_num = section_data.get("chapter_number")
    sec_num = section_data.get("section_number")
    if doc_id is not None and chap_num is not None and sec_num is not None:
        return f"{doc_id}::{chap_num}::{sec_num}"
    return None # Return None if essential parts are missing

def process_chapter_for_sections(
    chapter_data: Dict,
    client: Optional[OpenAI],
    processed_section_ids: set[str] # Added: Set of already processed IDs
    ) -> List[Dict]:
    """
    Identifies, enriches, and assembles section data for a single chapter,
    skipping sections that have already been processed.
    Returns only the newly processed sections for this chapter.
    """
    chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
    document_id = chapter_data.get("document_id", "UNKNOWN_DOC") # Get doc_id for ID creation
    logging.info(f"Processing sections for Chapter {chapter_number}...")
    newly_processed_sections = [] # Store only sections processed in this run
    recent_section_summaries = [] # Context for GPT within this chapter run

    initial_sections = split_chapter_into_sections(chapter_data)
    logging.info(f"  Identified {len(initial_sections)} initial sections for Chapter {chapter_number}.")
    skipped_count = 0

    for section_raw_data in tqdm(initial_sections, desc=f"Chapter {chapter_number} Sections"):
        section_number = section_raw_data["section_number"]

        # --- Check if already processed ---
        # Create a temporary dict to pass to the ID function
        temp_id_data = {
            "document_id": document_id,
            "chapter_number": chapter_number,
            "section_number": section_number
        }
        section_id = _create_section_id(temp_id_data)

        if section_id and section_id in processed_section_ids:
            logging.debug(f"  Skipping already processed section: {section_id}")
            skipped_count += 1
            continue # Skip this section

        logging.debug(f"  Processing Section {section_id} ('{section_raw_data.get('section_title', '')[:30]}...')")

        # --- Process Section (if not skipped) ---
        # Clean content and calculate tokens
        raw_slice = section_raw_data["raw_section_slice"]
        cleaned_content = clean_azure_tags(raw_slice)
        section_token_count = count_tokens(cleaned_content)

        if not cleaned_content.strip():
            logging.debug(f"  Skipping empty section {section_number} after cleaning.")
            continue

        # Determine section page range
        section_start_page, section_end_page = get_section_page_range(
            raw_slice, chapter_data.get("chapter_page_start", 0)
        )

        # Generate hierarchy string
        section_hierarchy = generate_hierarchy_string(section_raw_data)

        # Get GPT enrichment
        gpt_details = None
        if client:
            # Use only the last N summaries for context to limit prompt size
            context_summaries = recent_section_summaries[-MAX_RECENT_SUMMARIES_CONTEXT:]
            gpt_details = get_section_details_from_gpt(cleaned_content, chapter_data, context_summaries, client)
        else:
            logging.debug("  Skipping GPT enrichment (no client).")

        # Assemble final section data
        final_section_data = {
            # Carry over from chapter
            "document_id": chapter_data.get("document_id"),
            "chapter_number": chapter_number,
            "chapter_name": chapter_data.get("chapter_name"),
            "chapter_tags": chapter_data.get("chapter_tags"),
            "chapter_summary": chapter_data.get("chapter_summary"),
            "chapter_token_count": chapter_data.get("chapter_token_count"),
            # Section specific
            "section_number": section_number,
            "section_title": section_raw_data.get("section_title"),
            "section_hierarchy": section_hierarchy,
            "section_token_count": section_token_count,
            "section_start_page": section_start_page,
            "section_end_page": section_end_page,
            "cleaned_section_content": cleaned_content, # Pass cleaned content
            "raw_section_slice_start_pos": section_raw_data.get("start_pos"), # Pass raw positions
            "raw_section_slice_end_pos": section_raw_data.get("end_pos"),
            # From GPT (or defaults)
            "section_summary": gpt_details.get("section_summary", None) if gpt_details else None,
            "section_tags": gpt_details.get("section_tags", []) if gpt_details else [],
            "section_standard": gpt_details.get("section_standard", "N/A") if gpt_details else "N/A",
            "section_standard_codes": gpt_details.get("section_standard_codes", []) if gpt_details else [],
            "section_importance_score": gpt_details.get("section_importance_score", 0.5) if gpt_details else 0.5,
            "section_references": gpt_details.get("section_references", []) if gpt_details else [],
            # Add level_x hierarchy fields from raw data
            **{f"level_{i}": section_raw_data.get(f"level_{i}") for i in range(1, 7) if section_raw_data.get(f"level_{i}")}
        }
        processed_sections.append(final_section_data)

        # Update recent summaries list for context (only for successfully processed sections)
        if gpt_details and gpt_details.get("section_summary"):
            recent_section_summaries.append(gpt_details["section_summary"])
            # Keep only the last N summaries (handled by slicing in _build_section_prompt)

    if skipped_count > 0:
        logging.info(f"  Skipped {skipped_count} already processed sections in Chapter {chapter_number}.")
    logging.info(f"  Finished processing. Generated {len(newly_processed_sections)} new section records for Chapter {chapter_number}.")
    return newly_processed_sections # Return only the sections processed in this run


def run_stage2():
    """Main function to execute Stage 2 processing with resumability."""
    logging.info("--- Starting Stage 2: Section Identification & Enrichment ---")
    create_directory(OUTPUT_DIR)
    output_filepath = Path(OUTPUT_DIR) / OUTPUT_FILENAME

    # --- Load Existing Stage 2 Data (for Resumability) ---
    existing_section_data = []
    processed_section_ids = set()
    if output_filepath.exists():
        try:
            with open(output_filepath, "r", encoding="utf-8") as f:
                existing_section_data = json.load(f)
            if not isinstance(existing_section_data, list):
                 logging.warning(f"Existing output file {output_filepath} does not contain a valid list. Starting fresh.")
                 existing_section_data = []
            else:
                 # Create IDs for existing sections
                 for sec_data in existing_section_data:
                     sec_id = _create_section_id(sec_data)
                     if sec_id: processed_section_ids.add(sec_id)
                 logging.info(f"Loaded {len(existing_section_data)} existing section records from {output_filepath}. Found {len(processed_section_ids)} unique processed section IDs.")
        except json.JSONDecodeError:
            logging.error(f"Error decoding JSON from {output_filepath}. Starting fresh.", exc_info=True)
            existing_section_data = []; processed_section_ids = set()
        except Exception as e:
            logging.error(f"Error loading existing data from {output_filepath}: {e}. Starting fresh.", exc_info=True)
            existing_section_data = []; processed_section_ids = set()

    # --- Load Stage 1 Data ---
    stage1_output_file = Path(STAGE1_OUTPUT_DIR) / STAGE1_FILENAME
    if not stage1_output_file.exists():
        logging.error(f"Stage 1 output file not found: {stage1_output_file}. Cannot proceed.")
        # Exit if stage 1 data is missing
        sys.exit(f"Error: Stage 1 output file '{stage1_output_file}' not found.")
    try:
        with open(stage1_output_file, "r", encoding="utf-8") as f:
            all_chapter_data = json.load(f)
        logging.info(f"Loaded {len(all_chapter_data)} chapters from {stage1_output_file}")
    except Exception as e:
        logging.error(f"Error loading Stage 1 data from {stage1_output_file}: {e}", exc_info=True)
        # Exit if stage 1 data is unloadable
        sys.exit(f"Error: Failed to load Stage 1 data from '{stage1_output_file}'.")

    if not all_chapter_data:
        logging.warning("Stage 1 data is empty. No sections to process.")
        # Save existing data back if it was loaded, otherwise create empty file
        try:
            with open(output_filepath, "w", encoding="utf-8") as f:
                json.dump(existing_section_data, f, indent=2, ensure_ascii=False)
            logging.info(f"No chapters to process. Saved {len(existing_section_data)} existing section records back to {output_filepath}")
        except Exception as e:
            logging.error(f"Error saving output JSON to {output_filepath}: {e}", exc_info=True)
        return existing_section_data # Return existing data

    # --- Initialize OpenAI Client ---
    client = get_openai_client()
    if not client:
        logging.warning("OpenAI client initialization failed. Section enrichment will be skipped.")

    # --- Process Chapters for Sections ---
    # 'existing_section_data' will accumulate results
    total_new_sections_processed = 0
    total_sections_failed_this_run = 0 # Track failures if needed later

    for chapter_data in tqdm(all_chapter_data, desc="Processing Chapters for Sections"):
        chapter_number = chapter_data.get("chapter_number", "UNKNOWN")
        # Pass the set of processed IDs to the chapter processing function
        newly_processed_chapter_sections = process_chapter_for_sections(
            chapter_data, client, processed_section_ids
        )

        if newly_processed_chapter_sections:
            # Add newly processed sections to the main list
            existing_section_data.extend(newly_processed_chapter_sections)
            total_new_sections_processed += len(newly_processed_chapter_sections)
            # Update the set of processed IDs
            for sec_data in newly_processed_chapter_sections:
                sec_id = _create_section_id(sec_data)
                if sec_id: processed_section_ids.add(sec_id)

            # --- Incremental Save (after each chapter) ---
            try:
                # Sort before saving incrementally
                temp_sorted_data = existing_section_data
                if natsort:
                    try:
                        # Sort by chapter then section number
                        temp_sorted_data = sorted(existing_section_data, key=lambda x: (x.get('chapter_number', float('inf')), x.get('section_number', float('inf'))))
                    except Exception as sort_e:
                        logging.warning(f"Could not sort data before incremental save for chapter {chapter_number}: {sort_e}. Saving in current order.")
                        temp_sorted_data = existing_section_data # Fallback

                with open(output_filepath, "w", encoding="utf-8") as f:
                    json.dump(temp_sorted_data, f, indent=2, ensure_ascii=False)
                logging.debug(f"Incrementally saved {len(temp_sorted_data)} total section records after processing Chapter {chapter_number}")
                existing_section_data = temp_sorted_data # Update main list with sorted version

            except Exception as e:
                logging.error(f"Error during incremental save to {output_filepath} after processing Chapter {chapter_number}: {e}", exc_info=True)
                # Continue processing other chapters

    # --- Final Sort and Save ---
    logging.info("Performing final sort and save...")
    final_data_to_save = existing_section_data
    if natsort:
        try:
            final_data_to_save = sorted(existing_section_data, key=lambda x: (x.get('chapter_number', float('inf')), x.get('section_number', float('inf'))))
            logging.info("Performed final sort of section data.")
        except Exception as final_sort_e:
            logging.warning(f"Could not perform final sort: {final_sort_e}. Saving potentially unsorted data.")
            final_data_to_save = existing_section_data # Fallback
    else:
        logging.info("natsort not available, skipping final sort.")

    try:
        with open(output_filepath, "w", encoding="utf-8") as f:
            json.dump(final_data_to_save, f, indent=2, ensure_ascii=False)
        logging.info(f"Saved final data with {len(final_data_to_save)} section records to {output_filepath}")
    except Exception as e:
        logging.error(f"Error saving final output JSON to {output_filepath}: {e}", exc_info=True)


    # --- Print Summary ---
    final_record_count = len(final_data_to_save) # Use the length of the final list
    logging.info("--- Stage 2 Summary ---")
    logging.info(f"Total chapters from Stage 1: {len(all_chapter_data)}")
    # Note: Skipped count is logged per chapter now. Calculating total skipped requires summing across chapters or re-checking against input.
    logging.info(f"New sections processed this run: {total_new_sections_processed}")
    # logging.info(f"Sections failed this run: {total_sections_failed_this_run}") # Add if failure tracking is implemented
    logging.info(f"Total sections in final file : {final_record_count}")
    logging.info(f"Output JSON file             : {output_filepath}")
    logging.info("--- Stage 2 Finished ---")

    return final_data_to_save # Return the final data

# ==============================================================================
# Main Execution Block
# ==============================================================================

if __name__ == "__main__":
    run_stage2()
